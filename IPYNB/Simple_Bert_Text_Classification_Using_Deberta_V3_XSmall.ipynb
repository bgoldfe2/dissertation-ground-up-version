{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17EUnGuMD8zVFOn3cvMdFjYo7C_cdWcB-","timestamp":1677106793467}],"mount_file_id":"11QTZhH6bFJtK65OVTY8P9Y85on-ZEMlL","authorship_tag":"ABX9TyNlHqTMHwSMZ/Ked6vS0MU4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"6e2882981baa463497a1fb3155dc07bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4043b50f9a645c2a85b09f2e9e91a4a","IPY_MODEL_93c4dad68e5147dbbc6343b3fe86460e","IPY_MODEL_0ef8bab90c054cf984604ed4f80c7902"],"layout":"IPY_MODEL_e9fb4fedb5804d67bc26cbfdacfb33e1"}},"c4043b50f9a645c2a85b09f2e9e91a4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2523b1af8ece4f56b37cb396303f63f6","placeholder":"​","style":"IPY_MODEL_e84b4f63136f46a788d228cda03e8abd","value":"Downloading (…)okenizer_config.json: 100%"}},"93c4dad68e5147dbbc6343b3fe86460e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_11aa3c68275246df962892f8ae0c5a3c","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa3cdd631e0043cb893ef1ceffcb2161","value":52}},"0ef8bab90c054cf984604ed4f80c7902":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d31b35790e549f6bb5eee7ff9d4679e","placeholder":"​","style":"IPY_MODEL_49f690a71a66415a830c5e751341b265","value":" 52.0/52.0 [00:00&lt;00:00, 1.84kB/s]"}},"e9fb4fedb5804d67bc26cbfdacfb33e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2523b1af8ece4f56b37cb396303f63f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e84b4f63136f46a788d228cda03e8abd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11aa3c68275246df962892f8ae0c5a3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa3cdd631e0043cb893ef1ceffcb2161":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d31b35790e549f6bb5eee7ff9d4679e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49f690a71a66415a830c5e751341b265":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79cdce03bdbc47cfa14cfb047e324031":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b213b21e596a4770a1a8447bf88c77e1","IPY_MODEL_30540bc1684047d68634dfc91679dbff","IPY_MODEL_1898b45e16da4fe39a02e30e3b2899b8"],"layout":"IPY_MODEL_f724618de8b64c9bb0495f861f32620c"}},"b213b21e596a4770a1a8447bf88c77e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db968bac6bdf40db95ea12571bd3be84","placeholder":"​","style":"IPY_MODEL_13976f5093024d98b1e6e081dedb84e9","value":"Downloading (…)lve/main/config.json: 100%"}},"30540bc1684047d68634dfc91679dbff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cc21fab91664e28a0ba1ec4a1737855","max":578,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3262c9bea9294501b16d9ef0cbb4c79b","value":578}},"1898b45e16da4fe39a02e30e3b2899b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_079cbf89c58d44c7b3c45bfdfc4f733b","placeholder":"​","style":"IPY_MODEL_e979b7bc1c9b43578de9a8178c2fc2f1","value":" 578/578 [00:00&lt;00:00, 15.1kB/s]"}},"f724618de8b64c9bb0495f861f32620c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db968bac6bdf40db95ea12571bd3be84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13976f5093024d98b1e6e081dedb84e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5cc21fab91664e28a0ba1ec4a1737855":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3262c9bea9294501b16d9ef0cbb4c79b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"079cbf89c58d44c7b3c45bfdfc4f733b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e979b7bc1c9b43578de9a8178c2fc2f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b764cd5d298842b2bc3f63c3cf29a796":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_313ba32f64a84cdfa7cc34b7fd054922","IPY_MODEL_415dd610bfdb496da0422269afcad7bf","IPY_MODEL_6b9d84a12ef04948a80900eaed995da8"],"layout":"IPY_MODEL_a377026bcf414152b6eb17c31b59f7cb"}},"313ba32f64a84cdfa7cc34b7fd054922":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52435144637e429e9491abd2f81ca4d9","placeholder":"​","style":"IPY_MODEL_2f9cba1cbb714412a23abfff0bff2a42","value":"Downloading (…)&quot;spm.model&quot;;: 100%"}},"415dd610bfdb496da0422269afcad7bf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbdc73e27e7f4c79907deb45b3c13bd6","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73866f23937e46a29d923554ce030a2f","value":2464616}},"6b9d84a12ef04948a80900eaed995da8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1661dbff10a4e8bba8397ef8e540be4","placeholder":"​","style":"IPY_MODEL_d7adc945e9834b38978af77dcab4f1f9","value":" 2.46M/2.46M [00:00&lt;00:00, 24.4MB/s]"}},"a377026bcf414152b6eb17c31b59f7cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52435144637e429e9491abd2f81ca4d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f9cba1cbb714412a23abfff0bff2a42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbdc73e27e7f4c79907deb45b3c13bd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73866f23937e46a29d923554ce030a2f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1661dbff10a4e8bba8397ef8e540be4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7adc945e9834b38978af77dcab4f1f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"934b1ccdc2bf4452b6ca4f965f3031cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3f60d05e73348789ba18412dce763f1","IPY_MODEL_935c809f1b4040fb8c118089d39d43da","IPY_MODEL_ec87ba62063b47c0857d981b91d42cb6"],"layout":"IPY_MODEL_aa5d1464ab9b4b4e9cc0ee8f92d53136"}},"f3f60d05e73348789ba18412dce763f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10485d806bf941148fe8be1740e62ee7","placeholder":"​","style":"IPY_MODEL_f00b338567924682a46887158f304db7","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"935c809f1b4040fb8c118089d39d43da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f179d237ec3646c7ae1270a27c9ac789","max":286059269,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f325f239a4454626a41b7db373c6f694","value":286059269}},"ec87ba62063b47c0857d981b91d42cb6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ef1ef186c6d409ba757e5a8f2f1de7a","placeholder":"​","style":"IPY_MODEL_a7498e180416466d8c8ebdaf7e733c89","value":" 286M/286M [00:02&lt;00:00, 106MB/s]"}},"aa5d1464ab9b4b4e9cc0ee8f92d53136":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10485d806bf941148fe8be1740e62ee7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f00b338567924682a46887158f304db7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f179d237ec3646c7ae1270a27c9ac789":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f325f239a4454626a41b7db373c6f694":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ef1ef186c6d409ba757e5a8f2f1de7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7498e180416466d8c8ebdaf7e733c89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# This is the Fine-Tuning for Text Classification simple example\n","- https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894"],"metadata":{"id":"eLD0nCN1Ufok"}},{"cell_type":"code","source":["# put drive connection code if needed\n"],"metadata":{"id":"rTMPrUQbW_Rt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Code to install the nightly version if needed for debugging\n","\n","#!nvcc --version\n","#!pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu117"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sV4rdGMLpL3n","executionInfo":{"status":"ok","timestamp":1677264582851,"user_tz":300,"elapsed":5718,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"3c018a87-ae7d-4eb6-ca67-6408608724a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Tue_Mar__8_18:18:20_PST_2022\n","Cuda compilation tools, release 11.6, V11.6.124\n","Build cuda_11.6.r11.6/compiler.31057947_0\n","Looking in indexes: https://download.pytorch.org/whl/nightly/cu117, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FOjkyiXPUF3b","executionInfo":{"status":"ok","timestamp":1677271035611,"user_tz":300,"elapsed":25484,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"8e43b0a1-1a19-41ac-d481-d66950258c9d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5lz0BNh8T8Rm","executionInfo":{"status":"ok","timestamp":1677271046036,"user_tz":300,"elapsed":10429,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","#from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import AutoModelForSequenceClassification,AutoTokenizer\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","\n","from tabulate import tabulate\n","from tqdm import trange\n","import random\n","import math\n","\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""]},{"cell_type":"code","source":["!wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HXaLxOjOT9fn","executionInfo":{"status":"ok","timestamp":1677271046397,"user_tz":300,"elapsed":372,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"ff0c9ec5-9b42-4dc7-f34e-09832266c99b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-02-24 20:37:25--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 203415 (199K) [application/x-httpd-php]\n","Saving to: ‘smsspamcollection.zip’\n","\n","\rsmsspamcollection.z   0%[                    ]       0  --.-KB/s               \rsmsspamcollection.z 100%[===================>] 198.65K  --.-KB/s    in 0.1s    \n","\n","2023-02-24 20:37:26 (1.78 MB/s) - ‘smsspamcollection.zip’ saved [203415/203415]\n","\n"]}]},{"cell_type":"code","source":["!unzip -o smsspamcollection.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRYiLEFRUU2p","executionInfo":{"status":"ok","timestamp":1677271046599,"user_tz":300,"elapsed":206,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"55152bd2-5736-4551-8f02-4a910dc16fe7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  smsspamcollection.zip\n","  inflating: SMSSpamCollection       \n","  inflating: readme                  \n"]}]},{"cell_type":"code","source":["!head -10 SMSSpamCollection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpCzALb8UeN6","executionInfo":{"status":"ok","timestamp":1677271064990,"user_tz":300,"elapsed":326,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"d1d2f928-8f15-4d54-a1fa-784447c7e520"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n","ham\tOk lar... Joking wif u oni...\n","spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n","ham\tU dun say so early hor... U c already then say...\n","ham\tNah I don't think he goes to usf, he lives around here though\n","spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n","ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n","ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n","spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n","spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"]}]},{"cell_type":"code","source":["file_path = '/content/SMSSpamCollection'\n","df = pd.DataFrame({'label':int(), 'text':str()}, index = [])\n","with open(file_path) as f:\n","  for line in f.readlines():\n","    split = line.split('\\t')\n","    df = df.append({'label': 1 if split[0] == 'spam' else 0,\n","                    'text': split[1]},\n","                    ignore_index = True)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"Rtu_2nO0VLzg","executionInfo":{"status":"ok","timestamp":1677269841040,"user_tz":300,"elapsed":13216,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"cdb32e71-c539-4436-a39e-8b728f15880d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                               text\n","0      0  Go until jurong point, crazy.. Available only ...\n","1      0                    Ok lar... Joking wif u oni...\\n\n","2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n","3      0  U dun say so early hor... U c already then say...\n","4      0  Nah I don't think he goes to usf, he lives aro..."],"text/html":["\n","  <div id=\"df-cc5b9c0f-cde1-4d66-bc66-94ee8d95c9fa\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Ok lar... Joking wif u oni...\\n</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc5b9c0f-cde1-4d66-bc66-94ee8d95c9fa')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cc5b9c0f-cde1-4d66-bc66-94ee8d95c9fa button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cc5b9c0f-cde1-4d66-bc66-94ee8d95c9fa');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["text = df.text.values\n","labels = df.label.values"],"metadata":{"id":"Iai7SUzGVQ4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_nm = 'microsoft/deberta-v3-small'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_nm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202,"referenced_widgets":["6e2882981baa463497a1fb3155dc07bc","c4043b50f9a645c2a85b09f2e9e91a4a","93c4dad68e5147dbbc6343b3fe86460e","0ef8bab90c054cf984604ed4f80c7902","e9fb4fedb5804d67bc26cbfdacfb33e1","2523b1af8ece4f56b37cb396303f63f6","e84b4f63136f46a788d228cda03e8abd","11aa3c68275246df962892f8ae0c5a3c","fa3cdd631e0043cb893ef1ceffcb2161","8d31b35790e549f6bb5eee7ff9d4679e","49f690a71a66415a830c5e751341b265","79cdce03bdbc47cfa14cfb047e324031","b213b21e596a4770a1a8447bf88c77e1","30540bc1684047d68634dfc91679dbff","1898b45e16da4fe39a02e30e3b2899b8","f724618de8b64c9bb0495f861f32620c","db968bac6bdf40db95ea12571bd3be84","13976f5093024d98b1e6e081dedb84e9","5cc21fab91664e28a0ba1ec4a1737855","3262c9bea9294501b16d9ef0cbb4c79b","079cbf89c58d44c7b3c45bfdfc4f733b","e979b7bc1c9b43578de9a8178c2fc2f1","b764cd5d298842b2bc3f63c3cf29a796","313ba32f64a84cdfa7cc34b7fd054922","415dd610bfdb496da0422269afcad7bf","6b9d84a12ef04948a80900eaed995da8","a377026bcf414152b6eb17c31b59f7cb","52435144637e429e9491abd2f81ca4d9","2f9cba1cbb714412a23abfff0bff2a42","cbdc73e27e7f4c79907deb45b3c13bd6","73866f23937e46a29d923554ce030a2f","b1661dbff10a4e8bba8397ef8e540be4","d7adc945e9834b38978af77dcab4f1f9"]},"id":"TLGat00iVUzq","executionInfo":{"status":"ok","timestamp":1677269856437,"user_tz":300,"elapsed":9353,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"70603554-56d3-44a1-d094-12ffcb2053c4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e2882981baa463497a1fb3155dc07bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79cdce03bdbc47cfa14cfb047e324031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)\"spm.model\";:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b764cd5d298842b2bc3f63c3cf29a796"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"code","source":["print(type(tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VH7DtwaqiiV9","executionInfo":{"status":"ok","timestamp":1677269860297,"user_tz":300,"elapsed":319,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"59fe3a09-8a2e-40d4-9aed-0bddd4427799"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'transformers.models.deberta_v2.tokenization_deberta_v2_fast.DebertaV2TokenizerFast'>\n"]}]},{"cell_type":"code","source":["def print_rand_sentence():\n","  '''Displays the tokens and respective IDs of a random text sample'''\n","  index = random.randint(0, len(text)-1)\n","  table = np.array([tokenizer.tokenize(text[index]), \n","                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n","  print(tabulate(table,\n","                 headers = ['Tokens', 'Token IDs'],\n","                 tablefmt = 'fancy_grid'))\n","\n","print_rand_sentence()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zzqWr_EpVa88","executionInfo":{"status":"ok","timestamp":1677269863788,"user_tz":300,"elapsed":367,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"c4324a12-189c-4552-97d9-a11a3fb0480d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["╒═════════════╤═════════════╕\n","│ Tokens      │   Token IDs │\n","╞═════════════╪═════════════╡\n","│ ▁The        │         279 │\n","├─────────────┼─────────────┤\n","│ ▁LA         │        5240 │\n","├─────────────┼─────────────┤\n","│ Y           │        3021 │\n","├─────────────┼─────────────┤\n","│ ▁MAN        │       16536 │\n","├─────────────┼─────────────┤\n","│ !           │         300 │\n","├─────────────┼─────────────┤\n","│ ▁Just       │        1063 │\n","├─────────────┼─────────────┤\n","│ ▁to         │         264 │\n","├─────────────┼─────────────┤\n","│ ▁let        │         678 │\n","├─────────────┼─────────────┤\n","│ ▁you        │         274 │\n","├─────────────┼─────────────┤\n","│ ▁know       │         391 │\n","├─────────────┼─────────────┤\n","│ ▁you        │         274 │\n","├─────────────┼─────────────┤\n","│ ▁are        │         281 │\n","├─────────────┼─────────────┤\n","│ ▁missed     │        2854 │\n","├─────────────┼─────────────┤\n","│ ▁and        │         263 │\n","├─────────────┼─────────────┤\n","│ ▁thought    │         708 │\n","├─────────────┼─────────────┤\n","│ ▁off        │         442 │\n","├─────────────┼─────────────┤\n","│ .           │         260 │\n","├─────────────┼─────────────┤\n","│ ▁Do         │         771 │\n","├─────────────┼─────────────┤\n","│ ▁have       │         286 │\n","├─────────────┼─────────────┤\n","│ ▁a          │         266 │\n","├─────────────┼─────────────┤\n","│ ▁great      │         426 │\n","├─────────────┼─────────────┤\n","│ ▁day        │         406 │\n","├─────────────┼─────────────┤\n","│ .           │         260 │\n","├─────────────┼─────────────┤\n","│ ▁And        │         414 │\n","├─────────────┼─────────────┤\n","│ ▁if         │         337 │\n","├─────────────┼─────────────┤\n","│ ▁you        │         274 │\n","├─────────────┼─────────────┤\n","│ ▁can        │         295 │\n","├─────────────┼─────────────┤\n","│ ▁send       │        1339 │\n","├─────────────┼─────────────┤\n","│ ▁me         │         351 │\n","├─────────────┼─────────────┤\n","│ ▁b          │        2165 │\n","├─────────────┼─────────────┤\n","│ imbo        │       55975 │\n","├─────────────┼─────────────┤\n","│ ▁and        │         263 │\n","├─────────────┼─────────────┤\n","│ ▁u          │        3636 │\n","├─────────────┼─────────────┤\n","│ go          │        3452 │\n","├─────────────┼─────────────┤\n","│ '           │         280 │\n","├─────────────┼─────────────┤\n","│ s           │         268 │\n","├─────────────┼─────────────┤\n","│ ▁numbers    │        1687 │\n","├─────────────┼─────────────┤\n","│ ,           │         261 │\n","├─────────────┼─────────────┤\n","│ ▁ill        │        4643 │\n","├─────────────┼─────────────┤\n","│ ▁appreciate │        2685 │\n","├─────────────┼─────────────┤\n","│ .           │         260 │\n","├─────────────┼─────────────┤\n","│ ▁Safe       │        8244 │\n","╘═════════════╧═════════════╛\n"]}]},{"cell_type":"code","source":["token_id = []\n","attention_masks = []\n","\n","def preprocessing(input_text, tokenizer):\n","  '''\n","  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n","    - input_ids: list of token ids\n","    - token_type_ids: list of token type ids\n","    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n","  '''\n","  return tokenizer.encode_plus(\n","                        input_text,\n","                        add_special_tokens = True,\n","                        max_length = 32,\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,\n","                        return_tensors = 'pt'\n","                   )\n","\n","\n","for sample in text:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  token_id.append(encoding_dict['input_ids']) \n","  attention_masks.append(encoding_dict['attention_mask'])\n","\n","\n","token_id = torch.cat(token_id, dim = 0)\n","attention_masks = torch.cat(attention_masks, dim = 0)\n","labels = torch.tensor(labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hs9CF6TVkeX","executionInfo":{"status":"ok","timestamp":1677269870489,"user_tz":300,"elapsed":1359,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"4a9924fd-0712-4a0d-bd89-b2c5551c34a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["token_id[6]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y-CaLReyWYGz","executionInfo":{"status":"ok","timestamp":1677269875203,"user_tz":300,"elapsed":1051,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"c63b3d2c-2bd5-4a5d-bc76-938c59a457a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([    1,  1289,   312,  2387,   269,   298,   334,   264,  1828,   275,\n","          351,   260,   450,  2435,   351,   334, 12124,  6581,   260,     2,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["def print_rand_sentence_encoding():\n","  '''Displays tokens, token IDs and attention mask of a random text sample'''\n","  index = random.randint(0, len(text) - 1)\n","  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n","  token_ids = [i.numpy() for i in token_id[index]]\n","  attention = [i.numpy() for i in attention_masks[index]]\n","\n","  table = np.array([tokens, token_ids, attention]).T\n","  print(tabulate(table, \n","                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n","                 tablefmt = 'fancy_grid'))\n","\n","print_rand_sentence_encoding()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8HXLgfPCWdfJ","executionInfo":{"status":"ok","timestamp":1677269879419,"user_tz":300,"elapsed":302,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"23428c0a-5c93-4161-d1af-ac08f3111077"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["╒═══════════╤═════════════╤══════════════════╕\n","│ Tokens    │   Token IDs │   Attention Mask │\n","╞═══════════╪═════════════╪══════════════════╡\n","│ [CLS]     │           1 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁Congrats │       15378 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ !         │         300 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁Nokia    │       11764 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁3        │         404 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ 650       │       16992 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁video    │         750 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁camera   │        1822 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁phone    │         932 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁is       │         269 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁your     │         290 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁Call     │        2735 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁09       │        7993 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ 066       │       54479 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ 382       │       42394 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ 422       │       40640 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁Calls    │       30047 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁cost     │         751 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁150      │        3732 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ppm       │       59527 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁Ave      │        6089 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁call     │         660 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁3        │         404 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ mins      │       33199 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁vary     │        3958 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁from     │         292 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁mobile   │        1380 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ s         │         268 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁16       │         922 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ +         │        1186 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ ▁Close    │        8462 │                1 │\n","├───────────┼─────────────┼──────────────────┤\n","│ [SEP]     │           2 │                1 │\n","╘═══════════╧═════════════╧══════════════════╛\n"]}]},{"cell_type":"code","source":["val_ratio = 0.2\n","# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n","batch_size = 16\n","\n","# Indices of the train and validation splits stratified by labels\n","train_idx, val_idx = train_test_split(\n","    np.arange(len(labels)),\n","    test_size = val_ratio,\n","    shuffle = True,\n","    stratify = labels)\n","\n","# Train and validation sets\n","train_set = TensorDataset(token_id[train_idx], \n","                          attention_masks[train_idx], \n","                          labels[train_idx])\n","\n","val_set = TensorDataset(token_id[val_idx], \n","                        attention_masks[val_idx], \n","                        labels[val_idx])\n","\n","# Prepare DataLoader\n","train_dataloader = DataLoader(\n","            train_set,\n","            sampler = RandomSampler(train_set),\n","            batch_size = batch_size\n","        )\n","\n","validation_dataloader = DataLoader(\n","            val_set,\n","            sampler = SequentialSampler(val_set),\n","            batch_size = batch_size\n","        )"],"metadata":{"id":"-UY6ZhNQWn45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def b_tp(preds, labels):\n","  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n","  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n","\n","def b_fp(preds, labels):\n","  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n","  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n","\n","def b_tn(preds, labels):\n","  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n","  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n","\n","def b_fn(preds, labels):\n","  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n","  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n","\n","def b_metrics(preds, labels):\n","  '''\n","  Returns the following metrics:\n","    - accuracy    = (TP + TN) / N\n","    - precision   = TP / (TP + FP)\n","    - recall      = TP / (TP + FN)\n","    - specificity = TN / (TN + FP)\n","  '''\n","  preds = np.argmax(preds, axis = 1).flatten()\n","  labels = labels.flatten()\n","  tp = b_tp(preds, labels)\n","  tn = b_tn(preds, labels)\n","  fp = b_fp(preds, labels)\n","  fn = b_fn(preds, labels)\n","  b_accuracy = (tp + tn) / len(labels)\n","  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n","  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n","  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n","  return b_accuracy, b_precision, b_recall, b_specificity"],"metadata":{"id":"t9RO9ncOYH_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the SequenceClassification model\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_nm,\n","    num_labels = 2,\n","    output_attentions = False,\n","    output_hidden_states = False,\n",")\n","\n","# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n","optimizer = torch.optim.AdamW(model.parameters(), \n","                              lr = 5e-5,\n","                              eps = 1e-08\n","                              )\n","\n","# Run on GPU\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["934b1ccdc2bf4452b6ca4f965f3031cf","f3f60d05e73348789ba18412dce763f1","935c809f1b4040fb8c118089d39d43da","ec87ba62063b47c0857d981b91d42cb6","aa5d1464ab9b4b4e9cc0ee8f92d53136","10485d806bf941148fe8be1740e62ee7","f00b338567924682a46887158f304db7","f179d237ec3646c7ae1270a27c9ac789","f325f239a4454626a41b7db373c6f694","0ef1ef186c6d409ba757e5a8f2f1de7a","a7498e180416466d8c8ebdaf7e733c89"]},"id":"H2QNBChoYScs","executionInfo":{"status":"ok","timestamp":1677269924318,"user_tz":300,"elapsed":14019,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"111800b6-16e7-40fe-9b62-d8abfe77ca07"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/286M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"934b1ccdc2bf4452b6ca4f965f3031cf"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["DebertaV2ForSequenceClassification(\n","  (deberta): DebertaV2Model(\n","    (embeddings): DebertaV2Embeddings(\n","      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n","      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","      (dropout): StableDropout()\n","    )\n","    (encoder): DebertaV2Encoder(\n","      (layer): ModuleList(\n","        (0): DebertaV2Layer(\n","          (attention): DebertaV2Attention(\n","            (self): DisentangledSelfAttention(\n","              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (pos_dropout): StableDropout()\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaV2SelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaV2Intermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaV2Output(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (1): DebertaV2Layer(\n","          (attention): DebertaV2Attention(\n","            (self): DisentangledSelfAttention(\n","              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (pos_dropout): StableDropout()\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaV2SelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaV2Intermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaV2Output(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (2): DebertaV2Layer(\n","          (attention): DebertaV2Attention(\n","            (self): DisentangledSelfAttention(\n","              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (pos_dropout): StableDropout()\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaV2SelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaV2Intermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaV2Output(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (3): DebertaV2Layer(\n","          (attention): DebertaV2Attention(\n","            (self): DisentangledSelfAttention(\n","              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (pos_dropout): StableDropout()\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaV2SelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaV2Intermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaV2Output(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (4): DebertaV2Layer(\n","          (attention): DebertaV2Attention(\n","            (self): DisentangledSelfAttention(\n","              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (pos_dropout): StableDropout()\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaV2SelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaV2Intermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaV2Output(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","            (dropout): StableDropout()\n","          )\n","        )\n","        (5): DebertaV2Layer(\n","          (attention): DebertaV2Attention(\n","            (self): DisentangledSelfAttention(\n","              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (pos_dropout): StableDropout()\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaV2SelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaV2Intermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaV2Output(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","            (dropout): StableDropout()\n","          )\n","        )\n","      )\n","      (rel_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n","    )\n","  )\n","  (pooler): ContextPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): StableDropout()\n","  )\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): StableDropout()\n",")"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n","epochs = 2\n","\n","for _ in trange(epochs, desc = 'Epoch'):\n","    \n","    # ========== Training ==========\n","    \n","    # Set model to training mode\n","    model.train()\n","    \n","    # Tracking variables\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","\n","    for step, batch in enumerate(train_dataloader):\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        optimizer.zero_grad()\n","        # Forward pass\n","        train_output = model(b_input_ids, \n","                             token_type_ids = None, \n","                             attention_mask = b_input_mask, \n","                             labels = b_labels)\n","        # Backward pass\n","        train_output.loss.backward()\n","        optimizer.step()\n","        # Update tracking variables\n","        tr_loss += train_output.loss.item()\n","        nb_tr_examples += b_input_ids.size(0)\n","        nb_tr_steps += 1\n","\n","    # ========== Validation ==========\n","\n","    # Set model to evaluation mode\n","    model.eval()\n","\n","    # Tracking variables \n","    val_accuracy = []\n","    val_precision = []\n","    val_recall = []\n","    val_specificity = []\n","\n","    for batch in validation_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","          # Forward pass\n","          eval_output = model(b_input_ids, \n","                              token_type_ids = None, \n","                              attention_mask = b_input_mask)\n","        logits = eval_output.logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        # Calculate validation metrics\n","        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n","        val_accuracy.append(b_accuracy)\n","        # Update precision only when (tp + fp) !=0; ignore nan\n","        if b_precision != 'nan': val_precision.append(b_precision)\n","        # Update recall only when (tp + fn) !=0; ignore nan\n","        if b_recall != 'nan': val_recall.append(b_recall)\n","        # Update specificity only when (tn + fp) !=0; ignore nan\n","        if b_specificity != 'nan': val_specificity.append(b_specificity)\n","\n","    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n","    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n","    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n","    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n","    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zivAC2aQY8hp","executionInfo":{"status":"ok","timestamp":1677270038451,"user_tz":300,"elapsed":106605,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"409ba8ef-4dd5-4da9-d845-4487deea1ced"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]},{"output_type":"stream","name":"stderr","text":["Epoch:  50%|█████     | 1/2 [00:54<00:54, 54.76s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\t - Train loss: 0.1080\n","\t - Validation Accuracy: 0.9866\n","\t - Validation Precision: 0.9096\n","\t - Validation Recall: 0.9907\n","\t - Validation Specificity: 0.9869\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 2/2 [01:46<00:00, 53.04s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\t - Train loss: 0.0370\n","\t - Validation Accuracy: 0.9786\n","\t - Validation Precision: 0.9946\n","\t - Validation Recall: 0.8519\n","\t - Validation Specificity: 0.9989\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["new_sentence = 'WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.'\n","\n","# We need Token IDs and Attention Mask for inference on the new sentence\n","test_ids = []\n","test_attention_mask = []\n","\n","# Apply the tokenizer\n","encoding = preprocessing(new_sentence, tokenizer)\n","\n","# Extract IDs and Attention Mask\n","test_ids.append(encoding['input_ids'])\n","test_attention_mask.append(encoding['attention_mask'])\n","test_ids = torch.cat(test_ids, dim = 0)\n","test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n","\n","# Forward pass, calculate logit predictions\n","with torch.no_grad():\n","  output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n","\n","prediction = 'Spam' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Ham'\n","\n","print('Input Sentence: ', new_sentence)\n","print('Predicted Class: ', prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTgETXThZOPY","executionInfo":{"status":"ok","timestamp":1677270056160,"user_tz":300,"elapsed":574,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"02eec13e-7ecb-422c-f1a5-004c93ab5843"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Sentence:  WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n","Predicted Class:  Spam\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["len(test_ids[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UovfiJiRdVKo","executionInfo":{"status":"ok","timestamp":1677264773343,"user_tz":300,"elapsed":326,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"b754d38e-002a-4602-85cb-b62519500a88"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["encoding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qo4P6b74dXNq","executionInfo":{"status":"ok","timestamp":1677264775555,"user_tz":300,"elapsed":334,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"f8e5d9d0-5a0c-4b6f-f208-9e0511364ef1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    1, 67991,   300,   300,   463,   266,  6119,  1191,  1099,   274,\n","           286,   331,  2068,   264,  1069,   452, 28427,   962,  4597,  5505,\n","           300,   502,  1674,   660,  7993, 60489, 39908, 45500,   260, 21049,\n","          1197,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1]])}"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["output.logits.cpu().numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3pYnoAQdbCf","executionInfo":{"status":"ok","timestamp":1677264781092,"user_tz":300,"elapsed":302,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"6acfd806-5955-4cd8-8019-86b443774094"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-2.6940677,  2.7711425]], dtype=float32)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["type(np.argmax(output.logits.cpu().numpy()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_xye0nKgPeq","executionInfo":{"status":"ok","timestamp":1677264785052,"user_tz":300,"elapsed":304,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"38dea172-c759-47c4-ff2f-bc7bb98aa245"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.int64"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["np.argmax(output.logits.cpu().numpy()).flatten()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YFoLPmcfgloN","executionInfo":{"status":"ok","timestamp":1677264788881,"user_tz":300,"elapsed":301,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"6a55a5e3-5942-4c39-8bf9-7ac3cc14abad"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1])"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["np.argmax(output.logits.cpu().numpy()).flatten().item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m8hV-i4xg1oX","executionInfo":{"status":"ok","timestamp":1677264792423,"user_tz":300,"elapsed":300,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"bc61c066-19b0-4dfa-b2d7-e4697691613f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["type(np.argmax(output.logits.cpu().numpy()).flatten().item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d5nZyOPxg_hj","executionInfo":{"status":"ok","timestamp":1677264796311,"user_tz":300,"elapsed":326,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"813f2eb3-069d-41a0-99ac-f9d089149105"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["int"]},"metadata":{},"execution_count":34}]}]}